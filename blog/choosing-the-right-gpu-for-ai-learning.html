<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Choosing the Right GPU Setup for AI Engineering (and Learning) - Blog</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../post.css">
    <style>
        @view-transition {
            navigation: auto;
        }
    </style>
    <script src="../theme.js"></script>
    <script src="../transitions.js" defer></script>
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="../index.html" class="logo">dan.rio</a>
            <div class="nav-right">
                <ul class="nav-links">
                    <li><a href="../index.html" class="active">BLOG</a></li>
                    <li><a href="../about.html">ABOUT</a></li>
                </ul>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <line x1="12" y1="1" x2="12" y2="3"/>
                        <line x1="12" y1="21" x2="12" y2="23"/>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/>
                        <line x1="1" y1="12" x2="3" y2="12"/>
                        <line x1="21" y1="12" x2="23" y2="12"/>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <main class="container">
        <article class="post" style="view-transition-name: post-content-1;">
            <header class="post-header">
                <a href="../index.html" class="back-link">← Back to Blog</a>
                <h1 class="post-title-large" style="view-transition-name: post-title-1;">CHOOSING THE RIGHT GPU SETUP FOR AI ENGINEERING (AND LEARNING)</h1>
                <div class="post-meta" style="view-transition-name: post-date-1;">
                    <time class="post-date">October 20, 2025</time>
                    <span class="post-separator">•</span>
                    <span class="post-reading-time">10 min read</span>
                </div>
            </header>

            <div class="post-body">
                <p class="lead" style="view-transition-name: post-excerpt-1;">
                    A somewhat technical blog on how GPU choice shapes what you can actually learn about CUDA, distributed training, and training optimization and the realities of running a personal ML workstation.
                </p>
                <h1>Choosing the Right GPU Setup for AI Engineering (and Learning)</h1>
<p>This machine isn’t for winning benchmarks. It’s a lab for learning CUDA, optimization, and distributed systems through direct experience. That changes the selection criteria: peak FLOPs matter less than how many phenomena you can observe before hardware constraints distort the lesson. Under that lens, two RTX 5070 Ti cards beat a single flagship.</p>
<h2>TL;DR</h2>
<ul>
<li>Want to learn CUDA + DDP/FSDP and observe real distributed behavior? Prefer two 16 GB GPUs (e.g., dual 5070 Ti). You trade single‑GPU comfort for visibility into collectives, overlap, and sharding.</li>
<li>Want faster single‑GPU iteration and fewer VRAM constraints? A 4090/5090 is the comfort path.</li>
<li>Numbers below reflect public sources and documentation, not original hardware measurements.</li>
</ul>
<h2>Who is this for</h2>
<ul>
<li>Developers who want to study kernels, memory hierarchy, and distributed training in a controlled workstation.</li>
<li>Practitioners optimizing for learning breadth per watt and per hour, not for leaderboard results.</li>
</ul>
<h2>Decision matrix</h2>
<table>
<thead>
<tr>
<th>Goal</th>
<th>Constraints</th>
<th>Recommended setup</th>
<th>Why</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learn distributed training (DDP/FSDP), profile collectives</td>
<td>Reasonable cost, compact case</td>
<td>Dual 16 GB (5070 Ti or 5080 if price aligns)</td>
<td>Real process groups, measurable overlap and bucket behavior</td>
</tr>
<tr>
<td>Max single‑GPU headroom</td>
<td>Need larger context/batch, minimal friction</td>
<td>4090 or 5090</td>
<td>More VRAM and throughput, simpler thermals</td>
</tr>
<tr>
<td>Some collectives + comfort</td>
<td>Prefer big single GPU but want real NCCL semantics</td>
<td>4090 + helper CUDA card (e.g., A2000/3060)</td>
<td>Educational collectives with modest scaling; heterogeneous limits apply</td>
</tr>
</tbody>
</table>
<h2>What learning requires from hardware</h2>
<p>A workstation for learning must be capable of three modes. It must let you inspect kernels and memory hierarchy without constant resource starvation. It must expose real distributed behavior so you can reason about synchronization, sharding, and communication overlap with evidence rather than intuition. It must also be versatile enough to serve as a daily machine; reliability, thermals, and power delivery are part of the curriculum because instability invalidates results.</p>
<p>Sixteen gigabytes of VRAM on a modern NVIDIA GPU is sufficient for CUDA fundamentals, Nsight workflows, Triton and CUTLASS experiments, and meaningful training loops with medium models. Twenty‑four gigabytes increases comfort substantially: larger batch sizes, longer contexts, and fewer memory workarounds. Two GPUs do something different. They replace comfort with visibility. All‑reduce timing, bucket sizing, overlap with backward passes, sharded checkpoints, elastic restarts, and failure handling stop being theory and become measurements you can log and explain.</p>
<h3>VRAM and model sizing (quick guide)</h3>
<ul>
<li>~16 GB: comfortable for kernels, Nsight, Triton/CUTLASS, and ~0.7–1.0B param models in BF16 with small batches (context length matters).</li>
<li>~24 GB: larger batches/contexts; fewer memory workarounds; ~1.0–1.3B practical without offload.</li>
<li>32 GB+: more headroom; still, distributed algorithms are a different axis than sheer VRAM.</li>
</ul>
<h2>Architectures, precision, and features</h2>
<p>RTX 5070 Ti and RTX 5080 are Blackwell‑generation parts with fifth‑generation Tensor Cores and support for FP4 and FP6 in addition to FP8/16/32/64. RTX 4090 is Ada with fourth‑generation Tensor Cores, which means no FP4/6 but excellent FP8/16 throughput and a generous 24 GB of VRAM. RTX 5090 extends raw compute and VRAM further, but at a size and price that conflict with the goal of a compact, dual‑GPU lab. Precision features are interesting when you want to study numerical stability and efficiency; VRAM is decisive when you want to observe memory–compute–communication trade‑offs without artificial constraints.</p>
<blockquote>
<p>Notes on precision and interconnect</p>
<ul>
<li>FP4/FP6 appear in Blackwell hardware, but kernel/optimizer support in frameworks is still maturing. Expect staggered enablement.</li>
<li>Consumer RTX 40/50 series typically lack NVLink; multi‑GPU collectives run on PCIe. P2P can vary by platform/BIOS. Use <code>nvidia-smi topo -m</code> to inspect topology when you do have access to hardware.</li>
</ul>
</blockquote>
<h2>Performance relationships that matter for learning</h2>
<p>Exact framerates are irrelevant here. Relative scaling is enough to structure experiments and price sanity checks.</p>
<blockquote>
<p>Methods and assumptions (no local hardware)</p>
<ul>
<li>Source: consolidated public data (reviews, vendor docs, credible community benchmarks). No original measurements.</li>
<li>Workload lens: BF16 transformer training throughput; kernel maturity (FlashAttention, fused optimizers), drivers, and memory pressure shift numbers.</li>
<li>Interpretation: treat values as guides for experiment design, not purchasing promises.</li>
</ul>
</blockquote>
<p>Across public sources, a practical working model is: 5080 is roughly 13–15% faster than 5070 Ti on training‑relevant workloads, 4090 is about twice the 5070 Ti on BF16/FP32 math and roughly 50% ahead of 5080, and 5090 lifts another 50–70% over 5080 while adding more VRAM.</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th style="text-align: right;">VRAM</th>
<th>Tensor gen</th>
<th>Notable precision</th>
<th style="text-align: right;">Working relative ML throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 5070 Ti</td>
<td style="text-align: right;">16 GB</td>
<td>5th (Blackwell)</td>
<td>FP4/FP6/FP8/FP16</td>
<td style="text-align: right;">1.00 ×</td>
</tr>
<tr>
<td>RTX 5080</td>
<td style="text-align: right;">16 GB</td>
<td>5th (Blackwell)</td>
<td>FP4/FP6/FP8/FP16</td>
<td style="text-align: right;">~1.13–1.15 × vs 5070 Ti</td>
</tr>
<tr>
<td>RTX 4090</td>
<td style="text-align: right;">24 GB</td>
<td>4th (Ada)</td>
<td>FP8/FP16 (no FP4/6)</td>
<td style="text-align: right;">~2.0 × vs 5070 Ti; ~1.5 × vs 5080</td>
</tr>
<tr>
<td>RTX 5090</td>
<td style="text-align: right;">32 GB</td>
<td>Blackwell</td>
<td>FP4/FP6/FP8/FP16</td>
<td style="text-align: right;">~1.5–1.7 × vs 5080</td>
</tr>
</tbody>
</table>
<p><em>Table: Relative throughput for training‑oriented workloads. Variability is expected across frameworks and kernels; values reflect public internet sources as of 2025‑10.</em></p>
<p>The 4090’s additional 8 GB often matters more than its FLOPs. The ability to run 1–1.3B‑parameter transformers or longer contexts without offload is a qualitative change in the kinds of questions you can ask. Two 16 GB cards do not raise per‑rank capacity, but they let you study distributed algorithms on real hardware rather than by proxy.</p>
<h2>Local distributed training is the first laboratory; the cloud is the second</h2>
<p>Dual‑GPU on a single node gives you DDP and FSDP in conditions you control. Communication happens over PCIe. Latency is low, bandwidth is limited, and the step time decomposes cleanly into forward, backward, optimizer, and communication slices you can profile with Nsight Systems and the PyTorch profiler. You will see the effect of gradient bucket sizes, parameter sharding, prefetch and reshard policies, and whether compute and communication overlap as intended.</p>
<p>This knowledge transfers. When you move to a cloud node with NVLink or a cluster with NVSwitch or EFA/RDMA, the code does not change. The transport does. Hierarchical collectives become more attractive, bandwidth improves by an order of magnitude, and failure modes multiply. That is the correct time to measure scaling curves and sensitivity to topology. Without the local lab, the cloud only tells you that it is faster or slower; it does not tell you why.</p>
<h2>Form factor, thermals, and why a 3‑slot flagship complicates learning</h2>
<p>A single 4090 is simple if you never intend to run a second GPU. Its cooler is large, its transient power is high, and it occupies the physical space a second card would need for airflow. You can simulate multi‑process behavior with CUDA MPS and you can practice orchestration with Kubernetes time‑slicing, but you cannot form a meaningful multi‑GPU communicator without a second physical or MIG device. In contrast, most 5070 Ti boards are two‑slot designs that leave room for a second card and adequate intake. If the goal is to study synchronization rather than merely accelerate a single stream, that mechanical fact matters.</p>
<h2>Power delivery and stability considerations</h2>
<p>Two 5070 Ti cards and a modern high‑end CPU draw roughly 770–860 W sustained with 1.0–1.1 kW transient envelope. A 1000 W ATX 3.1 unit is the minimum that works; a 1200 W unit is the correct choice for quieter operation and margin. Each GPU should be on its own native 12V‑2×6 lead. Stability is not a vanity metric. It is the precondition for reproducible measurements and for debugging logic rather than chasing electrical noise.</p>
<h3>Minimal dual‑GPU lab BOM (example)</h3>
<ul>
<li>CPU with adequate lanes (e.g., 7950X/14900K class)</li>
<li>Motherboard with two x16 mechanical slots and good spacing</li>
<li>64–128 GB RAM</li>
<li>2 TB+ Gen4/5 NVMe SSD (useful for offload/FSDP checkpointing)</li>
<li>ATX 3.1 PSU 1000–1200 W, two native 12V‑2×6, separate leads per GPU</li>
<li>Case with front intake; two 2‑slot GPUs fit cleanly</li>
</ul>
<p>BIOS checklist: Above 4G Decoding, Resizable BAR, enable PCIe P2P if available.</p>
<h2>Price sanity through performance parity and elasticity</h2>
<p>Even when economics is not the driving factor, price should not violate basic proportionality. A useful parity check sets the 5070 Ti price as baseline and scales other targets by relative throughput. With a 5070 Ti reference near R$ 6,500, a strict proportional parity lands the 5080 around R$ 7,400 and the 5090 around R$ 11,300. Elasticity then discounts higher tiers to reflect diminishing educational returns and savings penalties, producing adjusted targets near R$ 7,000 for the 5080 and R$ 11,500 for the 5090, with buy‑now and no‑go bands around those anchors. These are not commandments but it is nice to try to avoid overpaing.</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th style="text-align: right;">Pure parity vs 5070 Ti</th>
<th style="text-align: right;">Elasticity‑adjusted optimal</th>
<th style="text-align: right;">Buy‑now band</th>
<th style="text-align: right;">No‑go band</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 5070 Ti</td>
<td style="text-align: right;">R$ 6,500</td>
<td style="text-align: right;">R$ 6,500</td>
<td style="text-align: right;">≤ R$ 5,850</td>
<td style="text-align: right;">≥ R$ 7,150</td>
</tr>
<tr>
<td>RTX 5080</td>
<td style="text-align: right;">~R$ 7,400</td>
<td style="text-align: right;">~R$ 7,000</td>
<td style="text-align: right;">≤ ~R$ 6,300</td>
<td style="text-align: right;">≥ ~R$ 7,700</td>
</tr>
<tr>
<td>RTX 5090</td>
<td style="text-align: right;">~R$ 11,300</td>
<td style="text-align: right;">~R$ 11,500</td>
<td style="text-align: right;">≤ ~R$ 10,350</td>
<td style="text-align: right;">≥ ~R$ 12,650</td>
</tr>
</tbody>
</table>
<p><em>Table: Pricing sanity relative to a 5070 Ti baseline. Brazil retail street prices as of 2025‑10; adjust to your market.</em></p>
<p>Ceilings serve a different role. They prevent weaker products from drifting into stronger price regions when a superior variant exists. Capping the entire 5080 family at R$ 8,500 and the 5090 at R$ 12,000 preserves the ordering of value even as street prices move.</p>
<h2>The asymmetric helper card compromise</h2>
<p>If a large 24 GB card is attractive for comfort but you still want genuine NCCL semantics locally, an asymmetrical arrangement is viable. A 4090 can be paired with a short, cool, low‑power CUDA card such as an RTX A2000 or a compact 3060/4060. The smaller device becomes the bottleneck and scaling will be modest, but process‑group creation, all‑reduce behavior, sharded state, and failure handling will all be real. When experiments depend on interconnect behavior or tensor parallelism, a cloud session is still required.</p>
<h2>Kubernetes, time‑slicing, and why sharding a 4090 is not MIG</h2>
<p>Consumer cards do not expose MIG. Kubernetes with the NVIDIA device plugin can time‑slice a single GPU across pods, but this does not create multiple CUDA devices with isolated VRAM. It is useful for rehearsal of manifests, logging, and artifact management. It is not a substitute for multi‑GPU collectives. For actual distributed algorithms, you need at least two physical GPUs or a datacenter GPU with MIG.</p>
<h2>The conclusion and its logic</h2>
<p>Two RTX 5070 Ti cards are the better laboratory. They maximize learning breadth per watt and per hour. They let you study CUDA deeply and then climb the stack into distributed training without leaving the workstation. They preserve enough mechanical space and electrical headroom to remain reliable. They carry Blackwell precision features for experiments in low‑bit training. They trade some single‑GPU comfort for access to the part of the system that is hardest to learn from a single device: how multiple processes coordinate work they cannot see. When the time comes to study fabrics and multi‑node scaling, the same code runs on a cloud node with different transport and the differences are measurable.</p>
<p>A single large GPU is still a legitimate choice for a different goal: fast iteration and fewer memory constraints. It is simply a different class of instrument. In a laboratory built to understand how the pieces fit rather than merely to accelerate them, dual mid‑range GPUs are the more instructive tool.</p>
            </div>
        </article>
    </main>

    <footer class="footer">
        <div class="footer-container">
            <div class="social-links">
                <a href="https://twitter.com" target="_blank" rel="noopener" aria-label="Twitter">
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                    </svg>
                </a>
                <a href="https://github.com" target="_blank" rel="noopener" aria-label="GitHub">
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                    </svg>
                </a>
                <a href="https://linkedin.com" target="_blank" rel="noopener" aria-label="LinkedIn">
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                </a>
            </div>
            <p class="copyright">© 2025 All Rights Reserved.</p>
        </div>
    </footer>
</body>
</html>